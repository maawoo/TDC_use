Testing of:
- Worker / Threads ratio
- Chunk sizes
- Chunk orientation

Results:
- Best to use 1 worker with many threads
"If your computations are mostly numeric in nature (for example NumPy and Pandas computations) and release the GIL entirely then it is advisable to run dask-worker processes with many threads and one process. This reduces communication costs and generally simplifies deployment." [1]

- Bigger chunks = less tasks = faster
Chunks with a few GB in size seem to work fine. I tested with a size of 1500x1500 here, but ended up using 3000x3000 chunks with a size of around 8.7 GB! I always kept an eye on memory consumption. Always hovered around 100 GB and no sudden spikes or any other unwanted events! Relevant: [2]

- Block chunks (e.g. 750x750) are much faster/efficient than band chunks (150xall)
This is a bit surprising because the data is actually oriented as bands interally (see [3] for more). However, this was just a very quick test. It's probably a good idea to test this further sometime and put a bit more thought into this. For now it works really well with block chunks, so I'll just continue with that :)
Relevant: [2]

    
[1] https://distributed.dask.org/en/latest/worker.html#thread-pool
[2] https://docs.dask.org/en/latest/array-best-practices.html?highlight=chunk#select-a-good-chunk-size
[3] https://force-eo.readthedocs.io/en/latest/howto/datacube.html#tut-datacube